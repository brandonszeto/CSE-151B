Why is multihead attention useful? Give one reason based on the fundamental architecture of multihead attention.

\begin{tcolorbox}
One reason multihead attention is useful is because the mutliple heads each learn different relationships between tokens in the input sequence. Each head focuses on different parts of the input sequence, gaining a more comprehensive understanding of the input. This can lead to better performance in tasks such as machine translation, text summarization, and language understanding.
\end{tcolorbox}
